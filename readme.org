* jeopardy
A bare-bones Jeopardy game for the browser.

* Cleaning and importing the data
The Jeopardy questions where sourced from the J-Archive. My particular
JSON dump is around 50MB, so importing into =postgres= will help us
manage all the question data.

To keep things simple, I'm going to generate a series of small JSON
files that can populate a complete Jeopardy board so that we can fetch
the questions from JavaScript without needing a database connection.

Each of these files should have a subset of categories where there is
at least one question for every point value.

It is important to match these point values to the given round and
air-date of the show so that the question's difficulty reflects the
awarded points.

| Years        | First Round | Second Round |
|--------------+-------------+--------------|
| 1984-2001    | $100-$500   | $200-$1,000  |
| 2001-Present | $200-$1,000 | $400-$2,000  |

Example Question JSON:
#+BEGIN_SRC text
{
  "category": "HISTORY",
  "air_date": "2004-12-31",
  "question": "'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'",
  "value": "$200",
  "answer": "Copernicus",
  "round": "Jeopardy!",
  "show_number": "4680"
}
#+END_SRC

** Converting the data
Convert the single-line array of JSON objects into a file containing
one JSON object per line so that postgres can ingest the data all at
once. (It's best to practice on a smaller file first)
#+BEGIN_SRC text
  # 0. Create smaller sample JSON file for testing. It's simpler to open
  # the smaller file in a text editor and clean things up manually.
  ~> head -c 1000 dump.json > practice.json
  ~> cat practice.json
  [{...}, {...}, {...}]

  # 1. Split into newlines and remove commas.
  # 2. Remove brackets from begining and end of file.
  ~> sed 's|\}, |\}\n|g' < dump.json | tr -d '[' | tr -d ']' > data.json
  ~> cat data.json
  {...}
  {...}
  {...}
#+END_SRC

** Creating SQL Tables
#+BEGIN_SRC sql
-- Import data using COPY with JSONB column
create table data_json (
  id integer primary key generated always as identity,
  data jsonb not null
);

-- Transfer data out of JSONB after importing
create table data (
  id integer primary key generated always as identity,
  category text,
  question text,
  answer text,
  value text,
  round text,
  show_number integer,
  air_date date
);
#+END_SRC

** Loading the data into postgres

First we need to import our JSON.
#+BEGIN_SRC text
  # sh
  ~> psql -U pguser -d jeopardy -c "copy data_json (data) from stdin;" < data.json

  # psql
  jeopardy=> select * from data_json;
  id  | data
  ----+------
    1 | {...}
    2 | {...}
    3 | {...}
#+END_SRC

Note: In our case, several JSON strings contain escaped double-quotes,
which will confuse the postgres =COPY= command. We can workaround this
issue with the following hack[fn:1] by Andrew Dunstan:
#+BEGIN_SRC text
  "copy data_json (data) from stdin csv quote e'\x01' delimiter e'\x02';"
#+END_SRC

By using the control characters =0x01= and =0x02=, we are telling
postgres to treat each line of JSON verbatim. This works because the
control characters will not exist within any of our valid JSON
objects, so postgres will not try to interpret any of our /real/
quotes (escaped or otherwise).

Now we can convert our JSONB into proper rows. (You can
always query the JSONB directly if needed.)
#+BEGIN_SRC sql
  insert into data (category, question, answer, value, round, show_number, air_date)
  (select r.*
  from data_json
  cross join lateral
    jsonb_to_record (data)
    as r(category text,
         question text,
         answer text,
         value text,
         round text,
         show_number integer,
         air_date date));
#+END_SRC

It can be helpful to turn on expanded display for checking the results:
#+BEGIN_SRC text
jeopardy=> \x
Expanded display is on.

jeopardy=> select * from data;
-[ RECORD 1 ]--------------------------------------------------------------------------------------------------------------
id          | 1
category    | HISTORY
question    | 'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'
answer      | Copernicus
value       | $200
round       | Jeopardy!
show_number | 4680
air_date    | 2004-12-31
#+END_SRC

* Normalizing the data
Rounds
#+BEGIN_SRC sql
create table rounds (id integer primary key, round text);
insert into rounds(id, round) values
  (1, 'Jeopardy!'),
  (2, 'Double Jeopardy!'),
  (3, 'Final Jeopardy!'),
  (4, 'Tiebreaker');
#+END_SRC

Categories
#+BEGIN_SRC sql
create table categories (id integer primary key, category text);

with c as (
  select distinct category from data order by category asc
)
insert into categories(id, category)
select row_number() over () as id, category from c;
#+END_SRC

* Questions
#+BEGIN_SRC sql
create table questions(
  id integer primary key generated always as identity,
  category_id integer references categories(id),
  round_id integer references rounds(id),
  question text,
  answer text,
  value text,
  show_number integer,
  air_date date
);
#+END_SRC

* Migrating the data
The first step is to add some columns and indexes to our =data= table.
This will help with querying the data we need to migrate into our new
tables.
#+BEGIN_SRC sql
alter table data add column category_id integer, add column round_id integer;
create index idx_date_round on data (round);
create index idx_data_category on data (category);
#+END_SRC

Now we can populate the existing rows with their soon-to-be foreign keys:
#+BEGIN_SRC sql
-- Set round_id
update data d
set round_id = r.id
from rounds r
where r.round = d.round

-- set category_id
update data d
set category_id = c.id
from categories c
where c.category = d.category
#+END_SRC

Finally, let's insert the data into our normalized tables:
#+BEGIN_SRC sql
insert into questions (category_id, round_id, question, answer, value, show_number, air_date)
select category_id, round_id, question, answer, value, show_number, air_date from data;
#+END_SRC

* Generating a complete set of Jeopardy questions
We can export a JSON file directly from =psql=.
#+BEGIN_SRC text
\t               -- Set tuples only to on (only print rows)
\a               -- Set output format as unaligned (no leading spaces)
\o jeopardy.json -- Output JSON file

with game as (
  select q.id, q.question, q.answer, q.value, r.round, c.category
  from questions q
  join rounds r on r.id = q.round_id
  join categories c on c.id = q.category_id
)
select row_to_json(game) from game limit 10;

#+END_SRC

* Footnotes
[fn:1] https://www.postgresql.org/message-id/54AD8CEF.3080904%40dunslane.net
#+BEGIN_QUOTE
CSV format, while not designed for this, is
nevertheless sufficiently flexible to allow successful import of json
data meeting certain criteria (essentially no newlines), like this:

copy the_table(jsonfield)
from '/path/to/jsondata'
csv quote e'\x01' delimiter e'\x02';

You aren't the first person to encounter this problem.
#+END_QUOTE
